---
title: Optimal Minimal Margin Maximization with Boosting
booktitle: Proceedings of the 36th International Conference on Machine Learning
year: '2019'
volume: '97'
address: 
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v97/mathiasen19a/mathiasen19a.pdf
url: http://proceedings.mlr.press/v97/mathiasen19a.html
abstract: Boosting algorithms iteratively produce linear combinations of more and
  more base hypotheses and it has been observed experimentally that the generalization
  error keeps improving even after achieving zero training error. One popular explanation
  attributes this to improvements in margins. A common goal in a long line of research,
  is to obtain large margins using as few base hypotheses as possible, culminating
  with the AdaBoostV algorithm by R{ä}tsch and Warmuth [JMLR’05]. The AdaBoostV algorithm
  was later conjectured to yield an optimal trade-off between number of hypotheses
  trained and the minimal margin over all training points (Nie, Warmuth, Vishwanathan
  and Zhang [JMLR’13]). Our main contribution is a new algorithm refuting this conjecture.
  Furthermore, we prove a lower bound which implies that our new algorithm is optimal.
layout: inproceedings
id: mathiasen19a
tex_title: Optimal Minimal Margin Maximization with Boosting
firstpage: 4392
lastpage: 4401
page: 4392-4401
order: 4392
cycles: false
bibtex_editor: Chaudhuri, Kamalika and Salakhutdinov, Ruslan
editor:
- given: Kamalika
  family: Chaudhuri
- given: Ruslan
  family: Salakhutdinov
bibtex_author: Mathiasen, Alexander and Larsen, Kasper Green and Gr{\o}nlund, Allan
author:
- given: Alexander
  family: Mathiasen
- given: Kasper Green
  family: Larsen
- given: Allan
  family: Grønlund
date: 2019-05-24
container-title: Proceedings of the 36th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 5
  - 24
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
