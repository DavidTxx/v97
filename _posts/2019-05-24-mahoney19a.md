---
title: Traditional and Heavy Tailed Self Regularization in Neural Network Models
booktitle: Proceedings of the 36th International Conference on Machine Learning
year: '2019'
volume: '97'
address: 
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v97/mahoney19a/mahoney19a.pdf
url: http://proceedings.mlr.press/v97/mahoney19a.html
abstract: Random Matrix Theory (RMT) is applied to analyze the weight matrices of
  Deep Neural Networks (DNNs), including both production quality, pre-trained models
  such as AlexNet and Inception, and smaller models trained from scratch, such as
  LeNet5 and a miniature-AlexNet. Empirical and theoretical results clearly indicate
  that the empirical spectral density (ESD) of DNN layer matrices displays signatures
  of traditionally-regularized statistical models, even in the absence of exogenously
  specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.
  Building on recent results in RMT, most notably its extension to Universality classes
  of Heavy-Tailed matrices, we develop a theory to identify <em>5+1 Phases of Training</em>,
  corresponding to increasing amounts of <em>Implicit Self-Regularization</em>. For
  smaller and/or older DNNs, this Implicit Self-Regularization is like traditional
  Tikhonov regularization, in that there is a “size scale” separating signal from
  noise. For state-of-the-art DNNs, however, we identify a novel form of <em>Heavy-Tailed
  Self-Regularization</em>, similar to the self-organization seen in the statistical
  physics of disordered systems. This implicit Self-Regularization can depend strongly
  on the many knobs of the training process. By exploiting the generalization gap
  phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases
  of training simply by changing the batch size.
layout: inproceedings
id: mahoney19a
tex_title: Traditional and Heavy Tailed Self Regularization in Neural Network Models
firstpage: 4284
lastpage: 4293
page: 4284-4293
order: 4284
cycles: false
bibtex_editor: Chaudhuri, Kamalika and Salakhutdinov, Ruslan
editor:
- given: Kamalika
  family: Chaudhuri
- given: Ruslan
  family: Salakhutdinov
bibtex_author: Mahoney, Michael and Martin, Charles
author:
- given: Michael
  family: Mahoney
- given: Charles
  family: Martin
date: 2019-05-24
container-title: Proceedings of the 36th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 5
  - 24
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
