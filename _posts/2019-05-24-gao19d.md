---
title: Demystifying Dropout
booktitle: Proceedings of the 36th International Conference on Machine Learning
year: '2019'
volume: '97'
address: 
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v97/gao19d/gao19d.pdf
url: http://proceedings.mlr.press/v97/gao19d.html
abstract: Dropout is a popular technique to train large-scale deep neural networks
  to alleviate the overfitting problem. To disclose the underlying reasons for its
  gain, numerous works have tried to explain it from different perspectives. In this
  paper, unlike existing works, we explore it from a new perspective to provide new
  insight into this line of research. In detail, we disentangle the forward and backward
  pass of dropout. Then, we find that these two passes need different levels of noise
  to improve the generalization performance of deep neural networks. Based on this
  observation, we propose the augmented dropout which employs different dropping strategies
  in the forward and backward pass. Experimental results have verified the effectiveness
  of our proposed method.
layout: inproceedings
id: gao19d
tex_title: Demystifying Dropout
firstpage: 2112
lastpage: 2121
page: 2112-2121
order: 2112
cycles: false
bibtex_editor: Chaudhuri, Kamalika and Salakhutdinov, Ruslan
editor:
- given: Kamalika
  family: Chaudhuri
- given: Ruslan
  family: Salakhutdinov
bibtex_author: Gao, Hongchang and Pei, Jian and Huang, Heng
author:
- given: Hongchang
  family: Gao
- given: Jian
  family: Pei
- given: Heng
  family: Huang
date: 2019-05-24
container-title: Proceedings of the 36th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 5
  - 24
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
