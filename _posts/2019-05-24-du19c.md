---
title: Gradient Descent Finds Global Minima of Deep Neural Networks
booktitle: Proceedings of the 36th International Conference on Machine Learning
year: '2019'
volume: '97'
address: 
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v97/du19c/du19c.pdf
url: http://proceedings.mlr.press/v97/du19c.html
abstract: Gradient descent finds a global minimum in training deep neural networks
  despite the objective function being non-convex. The current paper proves gradient
  descent achieves zero training loss in polynomial time for a deep over-parameterized
  neural network with residual connections (ResNet). Our analysis relies on the particular
  structure of the Gram matrix induced by the neural network architecture. This structure
  allows us to show the Gram matrix is stable throughout the training process and
  this stability implies the global optimality of the gradient descent algorithm.
  We further extend our analysis to deep residual convolutional neural networks and
  obtain a similar convergence result.
layout: inproceedings
id: du19c
tex_title: Gradient Descent Finds Global Minima of Deep Neural Networks
firstpage: 1675
lastpage: 1685
page: 1675-1685
order: 1675
cycles: false
bibtex_editor: Chaudhuri, Kamalika and Salakhutdinov, Ruslan
editor:
- given: Kamalika
  family: Chaudhuri
- given: Ruslan
  family: Salakhutdinov
bibtex_author: Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai,
  Xiyu
author:
- given: Simon
  family: Du
- given: Jason
  family: Lee
- given: Haochuan
  family: Li
- given: Liwei
  family: Wang
- given: Xiyu
  family: Zhai
date: 2019-05-24
container-title: Proceedings of the 36th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 5
  - 24
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v97/du19c/du19c-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
