---
title: "$\\textttDoubleSqueeze$: Parallel Stochastic Gradient Descent with Double-pass
  Error-Compensated Compression"
booktitle: Proceedings of the 36th International Conference on Machine Learning
year: '2019'
volume: '97'
address: 
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v97/tang19d/tang19d.pdf
url: http://proceedings.mlr.press/v97/tang19d.html
abstract: 'A standard approach in large scale machine learning is distributed stochastic
  gradient training, which requires the computation of aggregated stochastic gradients
  over multiple nodes on a network. Communication is a major bottleneck in such applications,
  and in recent years, compressed stochastic gradient methods such as QSGD (quantized
  SGD) and sparse SGD have been proposed to reduce communication. It was also shown
  that error compensation can be combined with compression to achieve better convergence
  in a scheme that each node compresses its local stochastic gradient and broadcast
  the result to all other nodes over the network in a single pass. However, such a
  single pass broadcast approach is not realistic in many practical implementations.
  For example, under the popular parameter-server model for distributed learning,
  the worker nodes need to send the compressed local gradients to the parameter server,
  which performs the aggregation. The parameter server has to compress the aggregated
  stochastic gradient again before sending it back to the worker nodes. In this work,
  we provide a detailed analysis on this two-pass communication model, with error-compensated
  compression both on the worker nodes and on the parameter server. We show that the
  error-compensated stochastic gradient algorithm admits three very nice properties:
  1) it is compatible with an <em>arbitrary</em> compression technique; 2) it admits
  an improved convergence rate than the non error-compensated stochastic gradient
  method such as QSGD and sparse SGD; 3) it admits linear speedup with respect to
  the number of workers. The empirical study is also conducted to validate our theoretical
  results.'
layout: inproceedings
id: tang19d
tex_title: "$\\texttt{DoubleSqueeze}$: Parallel Stochastic Gradient Descent with Double-pass
  Error-Compensated Compression"
firstpage: 6155
lastpage: 6165
page: 6155-6165
order: 6155
cycles: false
bibtex_editor: Chaudhuri, Kamalika and Salakhutdinov, Ruslan
editor:
- given: Kamalika
  family: Chaudhuri
- given: Ruslan
  family: Salakhutdinov
bibtex_author: Tang, Hanlin and Yu, Chen and Lian, Xiangru and Zhang, Tong and Liu,
  Ji
author:
- given: Hanlin
  family: Tang
- given: Chen
  family: Yu
- given: Xiangru
  family: Lian
- given: Tong
  family: Zhang
- given: Ji
  family: Liu
date: 2019-05-24
container-title: Proceedings of the 36th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 5
  - 24
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v97/tang19d/tang19d-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
