---
title: Efficient Training of BERT by Progressively Stacking
booktitle: Proceedings of the 36th International Conference on Machine Learning
year: '2019'
volume: '97'
address: 
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v97/gong19a/gong19a.pdf
url: http://proceedings.mlr.press/v97/gong19a.html
abstract: Unsupervised pre-training is popularly used in natural language processing.
  By designing proper unsupervised prediction tasks, a deep neural network can be
  trained and shown to be effective in many downstream tasks. As the data is usually
  adequate, the model for pre-training is generally huge and contains millions of
  parameters. Therefore, the training efficiency becomes a critical issue even when
  using high-performance hardware. In this paper, we explore an efficient training
  method for the state-of-the-art bidirectional Transformer (BERT) model. By visualizing
  the self-attention distribution of different layers at different positions in a
  well-trained BERT model, we find that in most layers, the self-attention distribution
  will concentrate locally around its position and the start-of-sentence token. Motivating
  from this, we propose the stacking algorithm to transfer knowledge from a shallow
  model to a deep model; then we apply stacking progressively to accelerate BERT training.
  The experimental results showed that the models trained by our training strategy
  achieve similar performance to models trained from scratch, but our algorithm is
  much faster.
layout: inproceedings
id: gong19a
tex_title: Efficient Training of {BERT} by Progressively Stacking
firstpage: 2337
lastpage: 2346
page: 2337-2346
order: 2337
cycles: false
bibtex_editor: Chaudhuri, Kamalika and Salakhutdinov, Ruslan
editor:
- given: Kamalika
  family: Chaudhuri
- given: Ruslan
  family: Salakhutdinov
bibtex_author: Gong, Linyuan and He, Di and Li, Zhuohan and Qin, Tao and Wang, Liwei
  and Liu, Tieyan
author:
- given: Linyuan
  family: Gong
- given: Di
  family: He
- given: Zhuohan
  family: Li
- given: Tao
  family: Qin
- given: Liwei
  family: Wang
- given: Tieyan
  family: Liu
date: 2019-05-24
container-title: Proceedings of the 36th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 5
  - 24
extras:
- label: Code
  link: https://github.com/gonglinyuan/StackingBERT
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
