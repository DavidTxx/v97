---
title: Width Provably Matters in Optimization for Deep Linear Neural Networks
booktitle: Proceedings of the 36th International Conference on Machine Learning
year: '2019'
volume: '97'
address: 
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v97/du19a/du19a.pdf
url: http://proceedings.mlr.press/v97/du19a.html
abstract: We prove that for an $L$-layer fully-connected linear neural network, if
  the width of every hidden layer is $\widetilde{\Omega}\left(L \cdot r \cdot d_{out}
  \cdot \kappa^3 \right)$, where $r$ and $\kappa$ are the rank and the condition number
  of the input data, and $d_{out}$ is the output dimension, then gradient descent
  with Gaussian random initialization converges to a global minimum at a linear rate.
  The number of iterations to find an $\epsilon$-suboptimal solution is $O(\kappa
  \log(\frac{1}{\epsilon}))$. Our polynomial upper bound on the total running time
  for wide deep linear networks and the $\exp\left(\Omega\left(L\right)\right)$ lower
  bound for narrow deep linear neural networks [Shamir, 2018] together demonstrate
  that wide layers are necessary for optimizing deep models.
layout: inproceedings
id: du19a
tex_title: Width Provably Matters in Optimization for Deep Linear Neural Networks
firstpage: 1655
lastpage: 1664
page: 1655-1664
order: 1655
cycles: false
bibtex_editor: Chaudhuri, Kamalika and Salakhutdinov, Ruslan
editor:
- given: Kamalika
  family: Chaudhuri
- given: Ruslan
  family: Salakhutdinov
bibtex_author: Du, Simon and Hu, Wei
author:
- given: Simon
  family: Du
- given: Wei
  family: Hu
date: 2019-05-24
container-title: Proceedings of the 36th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 5
  - 24
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v97/du19a/du19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
