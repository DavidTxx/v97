---
title: First-Order Algorithms Converge Faster than $O(1/k)$ on Convex Problems
booktitle: Proceedings of the 36th International Conference on Machine Learning
year: '2019'
volume: '97'
address: 
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v97/lee19e/lee19e.pdf
url: http://proceedings.mlr.press/v97/lee19e.html
abstract: It is well known that both gradient descent and stochastic coordinate descent
  achieve a global convergence rate of $O(1/k)$ in the objective value, when applied
  to a scheme for minimizing a Lipschitz-continuously differentiable, unconstrained
  convex function. In this work, we improve this rate to $o(1/k)$. We extend the result
  to proximal gradient and proximal coordinate descent on regularized problems to
  show similar $o(1/k)$ convergence rates. The result is tight in the sense that a
  rate of $O(1/k^{1+\epsilon})$ is not generally attainable for any $\epsilon>0$,
  for any of these methods.
layout: inproceedings
id: lee19e
tex_title: First-Order Algorithms Converge Faster than $O(1/k)$ on Convex Problems
firstpage: 3754
lastpage: 3762
page: 3754-3762
order: 3754
cycles: false
bibtex_editor: Chaudhuri, Kamalika and Salakhutdinov, Ruslan
editor:
- given: Kamalika
  family: Chaudhuri
- given: Ruslan
  family: Salakhutdinov
bibtex_author: Lee, Ching-Pei and Wright, Stephen
author:
- given: Ching-Pei
  family: Lee
- given: Stephen
  family: Wright
date: 2019-05-24
container-title: Proceedings of the 36th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 5
  - 24
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v97/lee19e/lee19e-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
