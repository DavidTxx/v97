---
title: Parameter efficient training of deep convolutional neural networks by dynamic
  sparse reparameterization
booktitle: Proceedings of the 36th International Conference on Machine Learning
year: '2019'
volume: '97'
address: 
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v97/mostafa19a/mostafa19a.pdf
url: http://proceedings.mlr.press/v97/mostafa19a.html
abstract: Modern deep neural networks are typically highly overparameterized. Pruning
  techniques are able to remove a significant fraction of network parameters with
  little loss in accuracy. Recently, techniques based on dynamic reallocation of non-zero
  parameters have emerged, allowing direct training of sparse networks without having
  to pre-train a large dense model. Here we present a novel dynamic sparse reparameterization
  method that addresses the limitations of previous techniques such as high computational
  cost and the need for manual configuration of the number of free parameters allocated
  to each layer. We evaluate the performance of dynamic reallocation methods in training
  deep convolutional networks and show that our method outperforms previous static
  and dynamic reparameterization methods, yielding the best accuracy for a fixed parameter
  budget, on par with accuracies obtained by iteratively pruning a pre-trained dense
  model. We further investigated the mechanisms underlying the superior generalization
  performance of the resultant sparse networks. We found that neither the structure,
  nor the initialization of the non-zero parameters were sufficient to explain the
  superior performance. Rather, effective learning crucially depended on the continuous
  exploration of the sparse network structure space during training. Our work suggests
  that exploring structural degrees of freedom during training is more effective than
  adding extra parameters to the network.
layout: inproceedings
id: mostafa19a
tex_title: Parameter efficient training of deep convolutional neural networks by dynamic
  sparse reparameterization
firstpage: 4646
lastpage: 4655
page: 4646-4655
order: 4646
cycles: false
bibtex_editor: Chaudhuri, Kamalika and Salakhutdinov, Ruslan
editor:
- given: Kamalika
  family: Chaudhuri
- given: Ruslan
  family: Salakhutdinov
bibtex_author: Mostafa, Hesham and Wang, Xin
author:
- given: Hesham
  family: Mostafa
- given: Xin
  family: Wang
date: 2019-05-24
container-title: Proceedings of the 36th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 5
  - 24
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v97/mostafa19a/mostafa19a-supp.pdf
- label: Code
  link: https://github.com/IntelAI/dynamic-reparameterization
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
