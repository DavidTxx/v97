---
title: Better generalization with less data using robust gradient descent
booktitle: Proceedings of the 36th International Conference on Machine Learning
year: '2019'
volume: '97'
address: 
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v97/holland19a/holland19a.pdf
url: http://proceedings.mlr.press/v97/holland19a.html
abstract: For learning tasks where the data (or losses) may be heavy-tailed, algorithms
  based on empirical risk minimization may require a substantial number of observations
  in order to perform well off-sample. In pursuit of stronger performance under weaker
  assumptions, we propose a technique which uses a cheap and robust iterative estimate
  of the risk gradient, which can be easily fed into any steepest descent procedure.
  Finite-sample risk bounds are provided under weak moment assumptions on the loss
  gradient. The algorithm is simple to implement, and empirical tests using simulations
  and real-world data illustrate that more efficient and reliable learning is possible
  without prior knowledge of the loss tails.
layout: inproceedings
id: holland19a
tex_title: Better generalization with less data using robust gradient descent
firstpage: 2761
lastpage: 2770
page: 2761-2770
order: 2761
cycles: false
bibtex_editor: Chaudhuri, Kamalika and Salakhutdinov, Ruslan
editor:
- given: Kamalika
  family: Chaudhuri
- given: Ruslan
  family: Salakhutdinov
bibtex_author: Holland, Matthew and Ikeda, Kazushi
author:
- given: Matthew
  family: Holland
- given: Kazushi
  family: Ikeda
date: 2019-05-24
container-title: Proceedings of the 36th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 5
  - 24
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v97/holland19a/holland19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
