---
title: Using Pre-Training Can Improve Model Robustness and Uncertainty
booktitle: Proceedings of the 36th International Conference on Machine Learning
year: '2019'
volume: '97'
address: 
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v97/hendrycks19a/hendrycks19a.pdf
url: http://proceedings.mlr.press/v97/hendrycks19a.html
abstract: He et al. (2018) have called into question the utility of pre-training by
  showing that training from scratch can often yield similar performance to pre-training.
  We show that although pre-training may not improve performance on traditional classification
  metrics, it improves model robustness and uncertainty estimates. Through extensive
  experiments on label corruption, class imbalance, adversarial examples, out-of-distribution
  detection, and confidence calibration, we demonstrate large gains from pre-training
  and complementary effects with task-specific methods. We show approximately a 10%
  absolute improvement over the previous state-of-the-art in adversarial robustness.
  In some cases, using pre-training without task-specific methods also surpasses the
  state-of-the-art, highlighting the need for pre-training when evaluating future
  methods on robustness and uncertainty tasks.
layout: inproceedings
id: hendrycks19a
tex_title: Using Pre-Training Can Improve Model Robustness and Uncertainty
firstpage: 2712
lastpage: 2721
page: 2712-2721
order: 2712
cycles: false
bibtex_editor: Chaudhuri, Kamalika and Salakhutdinov, Ruslan
editor:
- given: Kamalika
  family: Chaudhuri
- given: Ruslan
  family: Salakhutdinov
bibtex_author: Hendrycks, Dan and Lee, Kimin and Mazeika, Mantas
author:
- given: Dan
  family: Hendrycks
- given: Kimin
  family: Lee
- given: Mantas
  family: Mazeika
date: 2019-05-24
container-title: Proceedings of the 36th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 5
  - 24
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v97/hendrycks19a/hendrycks19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
