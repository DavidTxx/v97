---
title: Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning
booktitle: Proceedings of the 36th International Conference on Machine Learning
year: '2019'
volume: '97'
address: 
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v97/foerster19a/foerster19a.pdf
url: http://proceedings.mlr.press/v97/foerster19a.html
abstract: When observing the actions of others, humans make inferences about why they
  acted as they did, and what this implies about the world; humans also use the fact
  that their actions will be interpreted in this manner, allowing them to act informatively
  and thereby communicate efficiently with others. Although learning algorithms have
  recently achieved superhuman performance in a number of two-player, zero-sum games,
  scalable multi-agent reinforcement learning algorithms that can discover effective
  strategies and conventions in complex, partially observable settings have proven
  elusive. We present the <em>Bayesian action decoder</em> (BAD), a new multi-agent
  learning method that uses an approximate Bayesian update to obtain a public belief
  that conditions on the actions taken by all agents in the environment. BAD introduces
  a new Markov decision process, the <em>public belief MDP</em>, in which the action
  space consists of all deterministic partial policies, and exploits the fact that
  an agent acting only on this public belief state can still learn to use its private
  information if the action space is augmented to be over all partial policies mapping
  private information into environment actions. The Bayesian update is closely related
  to the <em>theory of mind</em> reasoning that humans carry out when observing othersâ€™
  actions. We first validate BAD on a proof-of-principle two-step matrix game, where
  it outperforms policy gradient methods; we then evaluate BAD on the challenging,
  cooperative partial-information card game Hanabi, where, in the two-player setting,
  it surpasses all previously published learning and hand-coded approaches, establishing
  a new state of the art.
layout: inproceedings
id: foerster19a
tex_title: "{B}ayesian Action Decoder for Deep Multi-Agent Reinforcement Learning"
firstpage: 1942
lastpage: 1951
page: 1942-1951
order: 1942
cycles: false
bibtex_editor: Chaudhuri, Kamalika and Salakhutdinov, Ruslan
editor:
- given: Kamalika
  family: Chaudhuri
- given: Ruslan
  family: Salakhutdinov
bibtex_author: Foerster, Jakob and Song, Francis and Hughes, Edward and Burch, Neil
  and Dunning, Iain and Whiteson, Shimon and Botvinick, Matthew and Bowling, Michael
author:
- given: Jakob
  family: Foerster
- given: Francis
  family: Song
- given: Edward
  family: Hughes
- given: Neil
  family: Burch
- given: Iain
  family: Dunning
- given: Shimon
  family: Whiteson
- given: Matthew
  family: Botvinick
- given: Michael
  family: Bowling
date: 2019-05-24
container-title: Proceedings of the 36th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 5
  - 24
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v97/foerster19a/foerster19a-supp.pdf
- label: Code
  link: https://bit.ly/2P3YOyd
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
